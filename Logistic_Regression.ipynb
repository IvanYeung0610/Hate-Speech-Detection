{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bba0791e",
   "metadata": {},
   "source": [
    "# Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69665607",
   "metadata": {},
   "source": [
    "### Install Libraries\n",
    "* **`wordcloud`** A word cloud generator in Python that we used to visualize our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c499501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\ivany\\anaconda3\\lib\\site-packages (1.9.4)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\ivany\\anaconda3\\lib\\site-packages (from wordcloud) (2.1.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\ivany\\anaconda3\\lib\\site-packages (from wordcloud) (11.1.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ivany\\anaconda3\\lib\\site-packages (from wordcloud) (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ivany\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ivany\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ivany\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ivany\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ivany\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\ivany\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ivany\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ivany\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e0c168",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78504108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import linear_model\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from wordcloud import STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3b1662",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25719292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (726119, 2)\n",
      "\n",
      "Class distribution:\n",
      "Label\n",
      "1    364525\n",
      "0    361594\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First few rows:\n",
      "                                             Content  Label\n",
      "0  denial of normal the con be asked to comment o...      1\n",
      "1  just by being able to tweet this insufferable ...      1\n",
      "2  that is retarded you too cute to be single tha...      1\n",
      "3  thought of a real badass mongol style declarat...      1\n",
      "4                                afro american basho      1\n"
     ]
    }
   ],
   "source": [
    "FILENAME = './HateSpeechDatasetBalanced.csv'\n",
    "df = pd.read_csv(FILENAME)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nClass distribution:\\n{df['Label'].value_counts()}\")\n",
    "print(f\"\\nFirst few rows:\\n{df.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c92aa7f",
   "metadata": {},
   "source": [
    "### Cleaning dataset\n",
    "* Remove punctuation\n",
    "* Remove links\n",
    "* Remove user mentions and hashtags\n",
    "* Remove numbers\n",
    "* Convert text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80274d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text cleaned.\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "df['Content'] = df['Content'].apply(clean_text)\n",
    "print(\"Dataset text cleaned.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1e2485",
   "metadata": {},
   "source": [
    "### Preprocessing the Data\n",
    "* Get 5000 examples from each class for the model\n",
    "* Get rid of words that have less than 100 occurrences in the training set and test (based on the training set in order to prevent cheating)\n",
    "* Get rid of stopwords from wordcloud library\n",
    "* Turn into a bag of words using one-hot encoding\n",
    "* Create Train/Test split for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a46e0ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in training data: 17,378\n",
      "Rare words removed from training and test set.\n",
      "One hot encoding mapping: {'actually': 0, 'address': 1, 'already': 2, 'always': 3, 'another': 4, 'anyone': 5, 'anything': 6, 'around': 7, 'article': 8, 'articles': 9, 'ass': 10, 'back': 11, 'believe': 12, 'better': 13, 'big': 14, 'bitch': 15, 'black': 16, 'blocked': 17, 'bullshit': 18, 'call': 19, 'come': 20, 'country': 21, 'day': 22, 'deleted': 23, 'deletion': 24, 'discussion': 25, 'done': 26, 'dont': 27, 'edit': 28, 'editing': 29, 'edits': 30, 'even': 31, 'every': 32, 'everyone': 33, 'face': 34, 'fact': 35, 'feel': 36, 'find': 37, 'first': 38, 'fuck': 39, 'fucking': 40, 'getting': 41, 'give': 42, 'go': 43, 'god': 44, 'going': 45, 'good': 46, 'got': 47, 'help': 48, 'history': 49, 'hope': 50, 'image': 51, 'information': 52, 'keep': 53, 'kill': 54, 'know': 55, 'let': 56, 'life': 57, 'list': 58, 'little': 59, 'look': 60, 'love': 61, 'made': 62, 'make': 63, 'man': 64, 'many': 65, 'mean': 66, 'means': 67, 'might': 68, 'much': 69, 'must': 70, 'name': 71, 'need': 72, 'never': 73, 'new': 74, 'non': 75, 'nothing': 76, 'now': 77, 'oh': 78, 'old': 79, 'one': 80, 'page': 81, 'pages': 82, 'people': 83, 'person': 84, 'place': 85, 'please': 86, 'point': 87, 'problem': 88, 'put': 89, 'question': 90, 'read': 91, 'real': 92, 'really': 93, 'retweet': 94, 'right': 95, 's': 96, 'said': 97, 'say': 98, 'section': 99, 'see': 100, 'shit': 101, 'slut': 102, 'someone': 103, 'something': 104, 'source': 105, 'sources': 106, 'state': 107, 'still': 108, 'stop': 109, 'stupid': 110, 'style': 111, 'sure': 112, 't': 113, 'take': 114, 'talk': 115, 'tell': 116, 'thank': 117, 'thanks': 118, 'thing': 119, 'think': 120, 'time': 121, 'trying': 122, 'two': 123, 'understand': 124, 'us': 125, 'use': 126, 'used': 127, 'user': 128, 'want': 129, 'well': 130, 'whatever': 131, 'white': 132, 'wiki': 133, 'wikipedia': 134, 'will': 135, 'without': 136, 'woman': 137, 'women': 138, 'work': 139, 'world': 140, 'wrong': 141}\n",
      "Converting training texts to vectors...\n",
      "Done.\n",
      "Converting test texts to vectors...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Only use 5000 samples from each class for modeling\n",
    "df_class_0 = df.query('Label == 0').sample(5000, random_state=42)\n",
    "df_class_1 = df.query('Label == 1').sample(5000, random_state=42)\n",
    "data = pd.concat([df_class_0, df_class_1])\n",
    "\n",
    "X_text = data['Content'].values\n",
    "y = data['Label'].values   \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_text, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Create vocabulary from training data only\n",
    "train_all_words = []\n",
    "for text in X_train:\n",
    "    words = str(text).lower().split()\n",
    "    train_all_words.extend(words)\n",
    "\n",
    "train_word_counts = Counter(train_all_words)\n",
    "print(f\"Total unique words in training data: {len(train_word_counts):,}\")\n",
    "\n",
    "MIN_WORD_FREQ = 100\n",
    "\n",
    "# Identify frequent words (appear at least MIN_WORD_FREQ times)\n",
    "freq_words = set([word for word, freq in train_word_counts.items() if freq >= MIN_WORD_FREQ ])\n",
    "\n",
    "\n",
    "def filter_rare_words(text, frequent_words_set):\n",
    "    words = str(text).split()\n",
    "    # Keep only words that are in the frequent_words set and not in STOPWORDS\n",
    "    filtered_words = [word for word in words if word in frequent_words_set and word not in STOPWORDS]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply filtering\n",
    "X_train_filtered = np.array([filter_rare_words(text, freq_words) for text in X_train])\n",
    "X_test_filtered = np.array([filter_rare_words(text, freq_words) for text in X_test])\n",
    "print(\"Rare words removed from training and test set.\")\n",
    "\n",
    "# One-hot encoding function\n",
    "def text_to_vector(text, word_to_index, vocab_size):\n",
    "    vector = np.zeros(vocab_size, dtype=np.int8)\n",
    "    \n",
    "    words = str(text).split()\n",
    "    for word in words:\n",
    "        if word in word_to_index:\n",
    "            idx = word_to_index[word]\n",
    "            vector[idx] = 1  # Binary: word present or not\n",
    "    \n",
    "    return vector\n",
    "\n",
    "\n",
    "# Create word for index mapping for one-hot encoding\n",
    "train_all_text_filtered = ' '.join(X_train_filtered)\n",
    "vocabulary = sorted(set(train_all_text_filtered.split()))\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "print(f\"One hot encoding mapping: {word_to_index}\")\n",
    "\n",
    "# Convert train and test to one-hot vectors\n",
    "print(\"Converting training texts to vectors...\")\n",
    "X_train_onehot = np.array([\n",
    "    text_to_vector(text, word_to_index, len(vocabulary)) \n",
    "    for text in X_train_filtered\n",
    "])\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Converting test texts to vectors...\")\n",
    "X_test_onehot = np.array([\n",
    "    text_to_vector(text, word_to_index, len(vocabulary)) \n",
    "    for text in X_test_filtered\n",
    "])\n",
    "print(\"Done.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54152ea6",
   "metadata": {},
   "source": [
    "### Logistic Regression with no Feature Transforamtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14a4f08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Shape: (6000, 142)\n",
      "Final Testing Shape: (4000, 142)\n",
      "Fitting model...\n",
      "Predicting...\n",
      "\n",
      "--- Results ---\n",
      "Train Accuracy: 0.7143\n",
      "Test Accuracy: 0.6885\n",
      "Precision: 0.6426\n",
      "Recall: 0.8074\n",
      "f1 score: 0.7157\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted: Unhateful</th>\n",
       "      <th>Predicted: Hateful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual: Unhateful</th>\n",
       "      <td>1186</td>\n",
       "      <td>872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual: Hateful</th>\n",
       "      <td>374</td>\n",
       "      <td>1568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Predicted: Unhateful  Predicted: Hateful\n",
       "Actual: Unhateful                  1186                 872\n",
       "Actual: Hateful                     374                1568"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Implement KNN using Sklearn\n",
    "print(f\"Final Training Shape: {X_train_onehot.shape}\")\n",
    "print(f\"Final Testing Shape: {X_test_onehot.shape}\")\n",
    "\n",
    "logreg = linear_model.LogisticRegression()\n",
    "\n",
    "print(\"Fitting model...\")\n",
    "logreg.fit(X_train_onehot, y_train)\n",
    "\n",
    "print(\"Predicting...\")\n",
    "y_pred_test = logreg.predict(X_test_onehot)\n",
    "y_pred_train = logreg.predict(X_train_onehot)\n",
    "\n",
    "# Evaluate\n",
    "train_acc = accuracy_score(y_train, y_pred_train)\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_pred_test, average='binary')\n",
    "\n",
    "print(\"\\n--- Results ---\")\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall: {rec:.4f}\")\n",
    "print(f\"f1 score: {f1:.4f}\")\n",
    "\n",
    "display(pd.DataFrame({\"Predicted: Unhateful\": confusion_matrix(y_test, y_pred_test)[:, 0], \n",
    "              \"Predicted: Hateful\": confusion_matrix(y_test, y_pred_test)[:, 1]},\n",
    "             index=['Actual: Unhateful', 'Actual: Hateful']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0781ddbb",
   "metadata": {},
   "source": [
    "### Logistic Regression with no Feature Transformation (Tuning K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "814d1497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Best Lambda: 1\n",
      "L2 Regularization prec:  [0.76025641 0.64262295]\n",
      "L2 Regularization recal:  [0.57628766 0.80741504]\n",
      "L2 Regularization fscore:  [0.65561083 0.71565495]\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter to consider\n",
    "# Lambda for regularization in Logistic Regression\n",
    "\n",
    "lambdas = [0.0000001, 0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "kf = model_selection.KFold(n_splits=5)\n",
    "\n",
    "scores = []\n",
    "\n",
    "# loop through and try all lambda values in lambdas array\n",
    "for lam in lambdas:\n",
    "    l2logreg = linear_model.LogisticRegression(penalty = 'l2', C=1/lam)\n",
    "    l2logreg.fit(X_train_onehot, y_train)\n",
    "    scores.append(np.mean(model_selection.cross_val_score(l2logreg, X_train_onehot, y_train, cv=kf, scoring='accuracy')))\n",
    "\n",
    "best_lambda = lambdas[np.argmax(scores)]\n",
    "\n",
    "print(f\"The Best Lambda: {best_lambda}\")\n",
    "l2logreg = linear_model.LogisticRegression(penalty = 'l2', C=1/best_lambda)\n",
    "l2logreg.fit(X_train_onehot, y_train)\n",
    "y_hat_l2logreg = l2logreg.predict(X_test_onehot)\n",
    "prec, recal, fscore, sup = precision_recall_fscore_support(y_test, y_hat_l2logreg)\n",
    "print('L2 Regularization prec: ', prec)\n",
    "print('L2 Regularization recal: ', recal)\n",
    "print('L2 Regularization fscore: ', fscore)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4956a6f0",
   "metadata": {},
   "source": [
    "### Logistic Regression with TF-IDF\n",
    "* Handles vocabulary selection\n",
    "* Removes stopwords (built-in to function)\n",
    "* Transforms raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05137f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Shape: (6000, 1000)\n",
      "Final Testing Shape: (4000, 1000)\n",
      "Fitting model...\n",
      "Predicting...\n",
      "\n",
      "--- Results ---\n",
      "Train Accuracy: 0.7143\n",
      "Test Accuracy: 0.6885\n",
      "Precision: 0.6426\n",
      "Recall: 0.8074\n",
      "f1 score: 0.7157\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted: Unhateful</th>\n",
       "      <th>Predicted: Hateful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual: Unhateful</th>\n",
       "      <td>1186</td>\n",
       "      <td>872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual: Hateful</th>\n",
       "      <td>374</td>\n",
       "      <td>1568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Predicted: Unhateful  Predicted: Hateful\n",
       "Actual: Unhateful                  1186                 872\n",
       "Actual: Hateful                     374                1568"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Implement Logistic Regression using Sklearn\n",
    "print(f\"Final Training Shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Final Testing Shape: {X_test_tfidf.shape}\")\n",
    "\n",
    "print(\"Fitting model...\")\n",
    "logreg.fit(X_train_onehot, y_train)\n",
    "\n",
    "print(\"Predicting...\")\n",
    "y_pred_test = logreg.predict(X_test_onehot)\n",
    "y_pred_train = logreg.predict(X_train_onehot)\n",
    "\n",
    "# Evaluate\n",
    "train_acc = accuracy_score(y_train, y_pred_train)\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_pred_test, average='binary')\n",
    "\n",
    "print(\"\\n--- Results ---\")\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall: {rec:.4f}\")\n",
    "print(f\"f1 score: {f1:.4f}\")\n",
    "\n",
    "display(pd.DataFrame({\"Predicted: Unhateful\": confusion_matrix(y_test, y_pred_test)[:, 0], \n",
    "              \"Predicted: Hateful\": confusion_matrix(y_test, y_pred_test)[:, 1]},\n",
    "             index=['Actual: Unhateful', 'Actual: Hateful']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ae7dcd",
   "metadata": {},
   "source": [
    "### Logistic Regression with TF-IDF (Tuning lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "93d47eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Best Lambda: 1\n",
      "L2 Regularization prec:  [0.76957001 0.69716377]\n",
      "L2 Regularization recal:  [0.67832847 0.78475798]\n",
      "L2 Regularization fscore:  [0.72107438 0.73837209]\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter to consider\n",
    "# Lambda for regularization in Logistic Regression\n",
    "# max_iter\n",
    "\n",
    "lambdas = [0.0000001, 0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "kf = model_selection.KFold(n_splits=5)\n",
    "\n",
    "scores = []\n",
    "\n",
    "# loop through and try all lambda values in lambdas array\n",
    "for lam in lambdas:\n",
    "    l2logreg = linear_model.LogisticRegression(max_iter = 1000, penalty = 'l2', C=1/lam)\n",
    "    l2logreg.fit(X_train_tfidf, y_train)\n",
    "    scores.append(np.mean(model_selection.cross_val_score(l2logreg, X_train_tfidf, y_train, cv=kf, scoring='accuracy')))\n",
    "\n",
    "best_lambda = lambdas[np.argmax(scores)]\n",
    "\n",
    "print(f\"The Best Lambda: {best_lambda}\")\n",
    "l2logreg = linear_model.LogisticRegression(penalty = 'l2', C=1/best_lambda)\n",
    "l2logreg.fit(X_train_tfidf, y_train)\n",
    "y_hat_l2logreg = l2logreg.predict(X_test_tfidf)\n",
    "prec, recal, fscore, sup = precision_recall_fscore_support(y_test, y_hat_l2logreg)\n",
    "print('L2 Regularization prec: ', prec)\n",
    "print('L2 Regularization recal: ', recal)\n",
    "print('L2 Regularization fscore: ', fscore)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
